{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d7b415",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-12T13:15:53.432468Z",
     "iopub.status.busy": "2025-12-12T13:15:53.431761Z",
     "iopub.status.idle": "2025-12-12T13:16:06.981091Z",
     "shell.execute_reply": "2025-12-12T13:16:06.980477Z"
    },
    "papermill": {
     "duration": 13.555077,
     "end_time": "2025-12-12T13:16:06.982473",
     "exception": false,
     "start_time": "2025-12-12T13:15:53.427396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8c0a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:06.988605Z",
     "iopub.status.busy": "2025-12-12T13:16:06.988257Z",
     "iopub.status.idle": "2025-12-12T13:16:07.047006Z",
     "shell.execute_reply": "2025-12-12T13:16:07.046129Z"
    },
    "papermill": {
     "duration": 0.062945,
     "end_time": "2025-12-12T13:16:07.048154",
     "exception": false,
     "start_time": "2025-12-12T13:16:06.985209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed6a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.054544Z",
     "iopub.status.busy": "2025-12-12T13:16:07.053992Z",
     "iopub.status.idle": "2025-12-12T13:16:07.059571Z",
     "shell.execute_reply": "2025-12-12T13:16:07.058875Z"
    },
    "papermill": {
     "duration": 0.009923,
     "end_time": "2025-12-12T13:16:07.060660",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.050737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalBirdDataset(Dataset):\n",
    "    \"\"\"dataset for multimodal learning: images + attribute embeddings\"\"\"\n",
    "    def __init__(self, df, img_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = f\"{self.img_dir}/{row['image_path']}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # get embedded attribute vector per image\n",
    "        attr_vec = torch.tensor(row['attr_vec'], dtype=torch.float32)\n",
    "\n",
    "        if self.is_test:\n",
    "            sample_id = int(row[\"id\"])\n",
    "            return image, attr_vec, sample_id\n",
    "        \n",
    "        label = int(row[\"label\"])\n",
    "        return image, attr_vec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4736f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.066430Z",
     "iopub.status.busy": "2025-12-12T13:16:07.066208Z",
     "iopub.status.idle": "2025-12-12T13:16:07.078126Z",
     "shell.execute_reply": "2025-12-12T13:16:07.077432Z"
    },
    "papermill": {
     "duration": 0.016303,
     "end_time": "2025-12-12T13:16:07.079320",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.063017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultimodalResNet18Skinny(nn.Module):\n",
    "    \"\"\"ResNet18 Skinny adapted for multimodal input (image + attributes)\"\"\"\n",
    "    def __init__(self, num_classes=200, attr_dim=312, dropout_rate=0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # image feature extraction\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(128, 256, 2, stride=2)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # image branch from ResNet\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # attribute branch processed\n",
    "        self.attr_fc = nn.Sequential(\n",
    "            nn.Linear(attr_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # combining both branches\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 256),  # 128 from image + 128 from attributes\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = [BasicBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_image, x_attr):\n",
    "        x = self.stem(x_image)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        image_features = self.image_fc(x)\n",
    "        \n",
    "        attr_features = self.attr_fc(x_attr)\n",
    "        \n",
    "        combined = torch.cat([image_features, attr_features], dim=1)\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/dataset-aml-feathers/aml-2025-feathers-in-focus/train_images.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/dataset-aml-feathers/aml-2025-feathers-in-focus/test_images_path.csv\")\n",
    "\n",
    "# transform labels\n",
    "train_df[\"label\"] = train_df[\"label\"] - 1\n",
    "\n",
    "# validation one sample per class\n",
    "val_df = train_df.groupby('label').apply(lambda x: x.sample(1, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "train_df = train_df.drop(val_df.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c6cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.240266Z",
     "iopub.status.busy": "2025-12-12T13:16:07.239897Z",
     "iopub.status.idle": "2025-12-12T13:16:07.245366Z",
     "shell.execute_reply": "2025-12-12T13:16:07.244669Z"
    },
    "papermill": {
     "duration": 0.009701,
     "end_time": "2025-12-12T13:16:07.246387",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.236686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same as ResNet18_skinny.ipynb\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedddings and align with dataframes\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_embeddings = np.load(\"/kaggle/input/embeddings-test-csv/train_embeddings.npy\")\n",
    "val_embeddings = np.load(\"/kaggle/input/embeddings-test-csv/val_embeddings.npy\")\n",
    "test_embeddings = np.load(\"/kaggle/input/embeddings-test-csv/test_embeddings.npy\")\n",
    "\n",
    "train_df[\"attr_vec\"] = [train_embeddings[i] for i in range(len(train_df))]\n",
    "val_df[\"attr_vec\"] = [val_embeddings[i] for i in range(len(val_df))]\n",
    "test_df[\"attr_vec\"] = [test_embeddings[i] for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab4cf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.390329Z",
     "iopub.status.busy": "2025-12-12T13:16:07.390122Z",
     "iopub.status.idle": "2025-12-12T13:16:07.395833Z",
     "shell.execute_reply": "2025-12-12T13:16:07.395166Z"
    },
    "papermill": {
     "duration": 0.01007,
     "end_time": "2025-12-12T13:16:07.396929",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.386859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load datasets and dataloaders\n",
    "train_ds = MultimodalBirdDataset(\n",
    "    train_df,\n",
    "    \"/kaggle/input/dataset-aml-feathers/aml-2025-feathers-in-focus/train_images\",\n",
    "    train_tfms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "val_ds = MultimodalBirdDataset(\n",
    "    val_df,\n",
    "    \"/kaggle/input/dataset-aml-feathers/aml-2025-feathers-in-focus/train_images\",\n",
    "    test_tfms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "test_ds = MultimodalBirdDataset(\n",
    "    test_df,\n",
    "    \"/kaggle/input/dataset-aml-feathers/aml-2025-feathers-in-focus/test_images\",\n",
    "    test_tfms,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fc64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.402927Z",
     "iopub.status.busy": "2025-12-12T13:16:07.402415Z",
     "iopub.status.idle": "2025-12-12T13:16:07.630504Z",
     "shell.execute_reply": "2025-12-12T13:16:07.629727Z"
    },
    "papermill": {
     "duration": 0.232559,
     "end_time": "2025-12-12T13:16:07.631953",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.399394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = MultimodalResNet18Skinny(num_classes=200, attr_dim=312, dropout_rate=0.6).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # higher lr has been tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796c64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.638243Z",
     "iopub.status.busy": "2025-12-12T13:16:07.638046Z",
     "iopub.status.idle": "2025-12-12T13:16:07.644146Z",
     "shell.execute_reply": "2025-12-12T13:16:07.643463Z"
    },
    "papermill": {
     "duration": 0.010472,
     "end_time": "2025-12-12T13:16:07.645210",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.634738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same asa in ResNet18_skinny.ipynb\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, attrs, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        attrs = attrs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, attrs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, attrs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            attrs = attrs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images, attrs)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21223556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:16:07.651240Z",
     "iopub.status.busy": "2025-12-12T13:16:07.650872Z",
     "iopub.status.idle": "2025-12-12T13:54:35.620321Z",
     "shell.execute_reply": "2025-12-12T13:54:35.619593Z"
    },
    "papermill": {
     "duration": 2307.973849,
     "end_time": "2025-12-12T13:54:35.621609",
     "exception": false,
     "start_time": "2025-12-12T13:16:07.647760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "epochs = 300\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_acc = evaluate(model, train_loader, device)\n",
    "\n",
    "    # validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, attrs, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            attrs = attrs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images, attrs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # monitor progress\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"../weights/multimodal_resnet18_skinny_best.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        break\n",
    "\n",
    "torch.save(model.state_dict(), \"../weights/multimodal_resnet18_skinny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd770d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.222783Z",
     "iopub.status.busy": "2025-12-12T13:54:36.222476Z",
     "iopub.status.idle": "2025-12-12T13:55:32.448437Z",
     "shell.execute_reply": "2025-12-12T13:55:32.447636Z"
    },
    "papermill": {
     "duration": 56.527324,
     "end_time": "2025-12-12T13:55:32.449636",
     "exception": false,
     "start_time": "2025-12-12T13:54:35.922312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, attrs, ids in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        images = images.to(device)\n",
    "        attrs = attrs.to(device)\n",
    "        logits = model(images, attrs)\n",
    "        preds = torch.argmax(logits, dim=1) + 1  # retranform to 1-200\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            predictions.append({\n",
    "                \"id\": int(ids[i].item()),\n",
    "                \"label\": int(preds[i].item())\n",
    "            })\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df = pred_df.sort_values('id').reset_index(drop=True)\n",
    "pred_df.to_csv(\"../submissions/submission_multimodal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c264ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:55:33.161563Z",
     "iopub.status.busy": "2025-12-12T13:55:33.160886Z",
     "iopub.status.idle": "2025-12-12T13:55:33.872728Z",
     "shell.execute_reply": "2025-12-12T13:55:33.871950Z"
    },
    "papermill": {
     "duration": 1.116438,
     "end_time": "2025-12-12T13:55:33.873945",
     "exception": false,
     "start_time": "2025-12-12T13:55:32.757507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(val_accs, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accuracy_curve.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8916869,
     "sourceId": 13990079,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8961555,
     "sourceId": 14127208,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2388.584072,
   "end_time": "2025-12-12T13:55:37.274298",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-12T13:15:48.690226",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
